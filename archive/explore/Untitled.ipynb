{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7cbc9dd-b8a9-4c38-a74c-d905774d497c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = 1\n",
    "y = (-x + 1) * 900\n",
    "y = 900\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dab6f14-a912-40dd-a511-f35a8a0a8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "B = 4\n",
    "labels = (torch.Tensor(list(range(2 * B))) + B) % (2 * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b660e481-48a7-42fb-84c9-a52e17d5cfab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6., 7., 0., 1., 2., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4a02bf-986c-448e-b3b5-ddd3fd7bff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True, False,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True,  True, False,  True],\n",
      "        [ True,  True,  True, False,  True,  True,  True, False],\n",
      "        [False,  True,  True,  True, False,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True,  True, False,  True],\n",
      "        [ True,  True,  True, False,  True,  True,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def mask_correlated_samples(batch_size, world_size):\n",
    "    N = 2 * batch_size * world_size\n",
    "    mask = torch.ones((N, N), dtype=bool)\n",
    "    mask = mask.fill_diagonal_(0)\n",
    "    for i in range(batch_size * world_size):\n",
    "        mask[i, batch_size + i] = 0\n",
    "        mask[batch_size + i, i] = 0\n",
    "    return mask\n",
    "\n",
    "batch_size, world_size = 4, 1\n",
    "mask = mask_correlated_samples(batch_size, world_size)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bf2a18be-3f06-4b76-ab37-755c8325b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True, False,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True,  True, False,  True],\n",
      "        [ True,  True,  True, False,  True,  True,  True, False],\n",
      "        [False,  True,  True,  True, False,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True,  True, False,  True],\n",
      "        [ True,  True,  True, False,  True,  True,  True, False]])\n",
      "tensor(0.4195)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def mask_correlated_samples(batch_size):\n",
    "    N = 2 * batch_size\n",
    "    mask = torch.ones((N, N), dtype=bool)\n",
    "    mask = mask.fill_diagonal_(0)\n",
    "    for i in range(batch_size):\n",
    "        mask[i, batch_size + i] = 0\n",
    "        mask[batch_size + i, i] = 0\n",
    "    print(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask(sequence_id):\n",
    "    b = len(sequence_id)\n",
    "    mask_id = sequence_id[None, :] - sequence_id[:, None]\n",
    "    mask_id = mask_id.reshape(-1)\n",
    "    one_indices = torch.arange(0, b * b, b + 1)\n",
    "    mask_id[one_indices] = 1\n",
    "    mask_id = mask_id.reshape(b, -1)\n",
    "    zero_id = torch.where(mask_id == 0, 0, 1)\n",
    "    return zero_id\n",
    "\n",
    "\n",
    "def contrastive_loss(x1, x2, mask):\n",
    "    assert x1.shape == x2.shape\n",
    "    B, _ = x1.shape\n",
    "    x = torch.cat([x1, x2], dim=0)\n",
    "    logits = nn.CosineSimilarity(dim=2)(x.unsqueeze(1), x.unsqueeze(0))\n",
    "    \n",
    "    neg_mask = mask_correlated_samples(B) * mask\n",
    "    pos_mask = (1 - neg_mask) * (1 - torch.eye(2 * B))\n",
    "\n",
    "    neg_logits = torch.exp(logits) * neg_mask\n",
    "    neg_logits = neg_logits.sum(1, keepdim=True)\n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_prob = logits - torch.log(exp_logits + neg_logits)\n",
    "    mean_log_prob_pos = (pos_mask * log_prob).sum(1) / mask.sum(1)\n",
    "    loss = -mean_log_prob_pos\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "x1 = torch.rand(4, 16)\n",
    "x2 = torch.rand(4, 16)\n",
    "y = contrastive_loss(x1, x1, torch.cat([get_mask(torch.tensor([1, 2, 1, 4, 5, 3, 4, 5]))], dim=0))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4d8813be-22a4-43f5-967f-56c3aa36d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9])\n",
      "tensor(1.8833)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def get_mask(sequence_id, n_views):\n",
    "    b = len(sequence_id)\n",
    "    mask_id = sequence_id[None, :] - sequence_id[:, None]\n",
    "    zero_id = torch.where(mask_id == 0, 1, 0)\n",
    "    one_indices = torch.cat([torch.arange(b // n_views) for i in range(n_views)], dim=0)\n",
    "    one_indices = (one_indices.unsqueeze(0) == one_indices.unsqueeze(1)).long()\n",
    "    mask = zero_id + one_indices\n",
    "    pos_mask = torch.where(mask > 0, 1, 0)\n",
    "    pos_mask.fill_diagonal_(0)\n",
    "    neg_mask = torch.where(mask > 0, 0, 1)\n",
    "    return pos_mask, neg_mask\n",
    "\n",
    "\n",
    "def contrastive_loss(x, sequence_id, n_views, temperature=1):\n",
    "    B, _ = x.shape\n",
    "    logits = nn.CosineSimilarity(dim=2)(x.unsqueeze(1), x.unsqueeze(0))\n",
    "    pos_mask, neg_mask = get_mask(sequence_id, n_views)\n",
    "    neg_logits = torch.exp(logits) * neg_mask\n",
    "    neg_logits = neg_logits.sum(1, keepdim=True)\n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_prob = logits - torch.log(exp_logits + neg_logits)\n",
    "    print(log_prob.shape)\n",
    "    mean_log_prob_pos = (pos_mask * log_prob).sum(1) / pos_mask.sum(1)\n",
    "    loss = -mean_log_prob_pos * temperature\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "x = torch.rand(9, 16)\n",
    "n_views = 3\n",
    "sequence_id = torch.tensor([1, 2, 1, 4, 5, 6, 7, 1, 0])\n",
    "y = contrastive_loss(x, sequence_id, n_views)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6f641-0a76-4530-95fa-d4cec8e28414",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/.config/nvim && mkdir patched-fonts\n",
    "cd patched-fonts\n",
    "git init\n",
    "git remote add origin https://github.com/ryanoasis/nerd-fonts\n",
    "git config core.sparsecheckout true\n",
    "echo \"Hack\" >> .git/info/sparse-checkout\n",
    "git pull --depth 1 origin master \n",
    "cd ..\n",
    "chmod +x install.sh\n",
    "./install.sh Hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f8452344-4fdb-41e3-8da1-62443246711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "base_idx = 450\n",
    "num_next_frames = 3\n",
    "frame_skip = 5\n",
    "num_frames = 900\n",
    "random_list = [\n",
    "            *np.arange(0, base_idx - num_next_frames * frame_skip),\n",
    "            *np.arange(base_idx + num_next_frames * frame_skip + 1, num_frames + 1)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3a28c59d-3341-44ca-baf1-8072a5a9cd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.cat([a for _ in range(3)], dim=0)\n",
    "b.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c4fc17fd-3f88-491c-85ca-d8c46baaa644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[494, 495, 496, 497, 498, 524, 525, 526, 527, 528, 554, 555, 556, 557, 558]\n",
      "15\n",
      "[148, 149, 150, 151, 152, 178, 179, 180, 181, 182, 208, 209, 210, 211, 212]\n",
      "15\n",
      "[357, 358, 359, 360, 361, 387, 388, 389, 390, 391, 417, 418, 419, 420, 421]\n",
      "15\n",
      "[381, 382, 383, 384, 385, 411, 412, 413, 414, 415, 441, 442, 443, 444, 445]\n",
      "15\n",
      "[353, 354, 355, 356, 357, 383, 384, 385, 386, 387, 413, 414, 415, 416, 417]\n",
      "15\n",
      "[449, 450, 451, 452, 453, 479, 480, 481, 482, 483, 509, 510, 511, 512, 513]\n",
      "15\n",
      "[797, 798, 799, 800, 801, 827, 828, 829, 830, 831, 857, 858, 859, 860, 861]\n",
      "15\n",
      "[806, 807, 808, 809, 810, 836, 837, 838, 839, 840, 866, 867, 868, 869, 870]\n",
      "15\n",
      "[375, 376, 377, 378, 379, 405, 406, 407, 408, 409, 435, 436, 437, 438, 439]\n",
      "15\n",
      "[269, 270, 271, 272, 273, 299, 300, 301, 302, 303, 329, 330, 331, 332, 333]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def _sample_indices(num_frame, num_segments, num_frames_per_segment, segment_duration):\n",
    "    \"\"\"\n",
    "    Samples indices for segments.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    time_duration = (num_segments - 1) * segment_duration + num_frames_per_segment\n",
    "    assert time_duration < num_frame // 2, \"Not enough frames to sample\"\n",
    "    assert num_segments % 2 == 1\n",
    "    center_frame_idx = random.randint(0, num_frame)\n",
    "    left_frame_idx = center_frame_idx -  segment_duration * num_segments // 2 - num_frames_per_segment // 2\n",
    "    left_frame_idxs = [left_frame_idx + i * segment_duration for i in range(0, num_segments)]\n",
    "    for i in left_frame_idxs:\n",
    "        for j in range(num_frames_per_segment):\n",
    "            indices.append(i + j)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    r = _sample_indices(900, 3, 5, 30)\n",
    "    print(len(r))\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a2d87354-47b4-46e7-bd55-729b61d70a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(0, 12)\n",
    "a.reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "36d88597-bf38-4084-a801-82ce287a3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200 526 659]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample(video_list, new_length, num_segments):\n",
    "    if((len(video_list) - new_length + 1) < num_segments):\n",
    "        average_duration = (len(video_list) - 5 + 1) // (num_segments)\n",
    "    else:\n",
    "        average_duration = (len(video_list) - new_length + 1) // (num_segments)\n",
    "    offsets = []\n",
    "    if average_duration > 0:\n",
    "        offsets += list(np.multiply(list(range(num_segments)), average_duration) + np.random.randint(average_duration,size=num_segments))\n",
    "    elif len(video_list) > num_segments:\n",
    "        if((len(video_list) - new_length + 1) >= num_segments):\n",
    "            offsets += list(np.sort(np.random.randint(len(video_list) - new_length + 1, size=num_segments)))\n",
    "        else:\n",
    "            offsets += list(np.sort(np.random.randint(len(video_list) - 5 + 1, size=num_segments)))\n",
    "    else:\n",
    "        offsets += list(np.zeros((num_segments,)))\n",
    "    offsets = np.array(offsets)\n",
    "    return offsets + 1\n",
    "\n",
    "\n",
    "\n",
    "video_list = list(range(900))\n",
    "new_length = 5\n",
    "num_segments = 3\n",
    "offsets = sample(video_list, new_length, num_segments)\n",
    "print(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6fd40b38-a533-46ff-b191-7baa102d1b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "M = 4\n",
    "N = 3\n",
    "\n",
    "a = torch.ones((M, M)).view(M, M, 1, 1)\n",
    "b = torch.eye(N).view(1, 1, N, N)\n",
    "masks = (a * b).transpose(1, 2).reshape(M * N, M * N)\n",
    "print(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad8a10e-1cb8-4d0c-bb40-5b18ff313af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1060)\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "ce = nn.CrossEntropyLoss(reduction='none')\n",
    "x = torch.rand(256, 65536)\n",
    "y = torch.randint(0, 65536, (256,))\n",
    "loss = ce(x, y)\n",
    "print(loss.mean())\n",
    "print(loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24e9c9e7-8772-4283-8d6a-47cfb9346f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor(1).uniform_(0, 1)\n",
    "# a=torch.Tensor(2,3).uniform_(-1,1)\n",
    "torch.tensor([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116d2c3-9091-4212-839c-6ac0cee13043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ssl_sequence(sequence: tf.Tensor,\n",
    "                        num_steps: int,\n",
    "                        random: bool,\n",
    "                        stride: int = 1,\n",
    "                        num_windows: Optional[int] = 2) -> tf.Tensor:\n",
    "  \"\"\"Samples two segments of size num_steps randomly from a given sequence.\n",
    "\n",
    "  Currently it only supports images, and specically designed for video self-\n",
    "  supervised learning.\n",
    "\n",
    "  Args:\n",
    "    sequence: Any tensor where the first dimension is timesteps.\n",
    "    num_steps: Number of steps (e.g. frames) to take.\n",
    "    random: A boolean indicating whether to random sample the single window. If\n",
    "      True, the offset is randomized. Only True is supported.\n",
    "    stride: Distance to sample between timesteps.\n",
    "    num_windows: Number of sequence sampled.\n",
    "\n",
    "  Returns:\n",
    "    A single Tensor with first dimension num_steps with the sampled segment.\n",
    "  \"\"\"\n",
    "  sequence_length = tf.shape(sequence)[0]\n",
    "  sequence_length = tf.cast(sequence_length, tf.float32)\n",
    "  if random:\n",
    "    max_offset = tf.cond(\n",
    "        tf.greater(sequence_length, (num_steps - 1) * stride),\n",
    "        lambda: sequence_length - (num_steps - 1) * stride,\n",
    "        lambda: sequence_length)\n",
    "\n",
    "    max_offset = tf.cast(max_offset, dtype=tf.float32)\n",
    "    def cdf(k, power=1.0):\n",
    "      \"\"\"Cumulative distribution function for x^power.\"\"\"\n",
    "      p = -tf.math.pow(k, power + 1) / (\n",
    "          power * tf.math.pow(max_offset, power + 1)) + k * (power + 1) / (\n",
    "              power * max_offset)\n",
    "      return p\n",
    "\n",
    "    u = tf.random.uniform(())\n",
    "    k_low = tf.constant(0, dtype=tf.float32)\n",
    "    k_up = max_offset\n",
    "    k = tf.math.floordiv(max_offset, 2.0)\n",
    "\n",
    "    c = lambda k_low, k_up, k: tf.greater(tf.math.abs(k_up - k_low), 1.0)\n",
    "    # pylint:disable=g-long-lambda\n",
    "    b = lambda k_low, k_up, k: tf.cond(\n",
    "        tf.greater(cdf(k), u),\n",
    "        lambda: [k_low, k, tf.math.floordiv(k + k_low, 2.0)],\n",
    "        lambda: [k, k_up, tf.math.floordiv(k_up + k, 2.0)])\n",
    "\n",
    "    _, _, k = tf.while_loop(c, b, [k_low, k_up, k])\n",
    "    delta = tf.cast(k, tf.int32)\n",
    "    max_offset = tf.cast(max_offset, tf.int32)\n",
    "    sequence_length = tf.cast(sequence_length, tf.int32)\n",
    "\n",
    "    choice_1 = tf.cond(\n",
    "        tf.equal(max_offset, sequence_length),\n",
    "        lambda: tf.random.uniform((),\n",
    "                                  maxval=tf.cast(max_offset, dtype=tf.int32),\n",
    "                                  dtype=tf.int32),\n",
    "        lambda: tf.random.uniform((),\n",
    "                                  maxval=tf.cast(max_offset - delta,\n",
    "                                                 dtype=tf.int32),\n",
    "                                  dtype=tf.int32))\n",
    "    choice_2 = tf.cond(\n",
    "        tf.equal(max_offset, sequence_length),\n",
    "        lambda: tf.random.uniform((),\n",
    "                                  maxval=tf.cast(max_offset, dtype=tf.int32),\n",
    "                                  dtype=tf.int32),\n",
    "        lambda: choice_1 + delta)\n",
    "    # pylint:disable=g-long-lambda\n",
    "    shuffle_choice = tf.random.shuffle((choice_1, choice_2))\n",
    "    offset_1 = shuffle_choice[0]\n",
    "    offset_2 = shuffle_choice[1]\n",
    "\n",
    "  else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "  indices_1 = _sample_or_pad_sequence_indices(\n",
    "      sequence=sequence,\n",
    "      num_steps=num_steps,\n",
    "      stride=stride,\n",
    "      offset=offset_1)\n",
    "\n",
    "  indices_2 = _sample_or_pad_sequence_indices(\n",
    "      sequence=sequence,\n",
    "      num_steps=num_steps,\n",
    "      stride=stride,\n",
    "      offset=offset_2)\n",
    "\n",
    "  indices = tf.concat([indices_1, indices_2], axis=0)\n",
    "  indices.set_shape((num_windows * num_steps,))\n",
    "  output = tf.gather(sequence, indices)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "968708c2-b728-4e40-9e8e-6f274904683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1, 2])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2]])\n",
      "tensor([[0, 1, 1, 2, 2],\n",
      "        [2, 0, 2, 0, 1],\n",
      "        [0, 2, 2, 1, 2],\n",
      "        [1, 0, 1, 0, 2]])\n",
      "tensor([[ True, False, False,  True,  True],\n",
      "        [False,  True, False,  True,  True],\n",
      "        [ True,  True,  True, False,  True],\n",
      "        [ True,  True,  True,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "b = 4\n",
    "k = 5\n",
    "pos = torch.randint(0, 3, (b,))\n",
    "bank = torch.randint(0, 3, (b, k))\n",
    "print(pos)\n",
    "print(pos.unsqueeze(1).repeat(1, k))\n",
    "print(bank)\n",
    "mask = bank - pos.unsqueeze(1).repeat(1, k)\n",
    "mask = mask != 0\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d6b2e2-1ffb-4969-86f3-e2ad8a9f06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "encoder_q = torchvision.models.resnet50(pretrained=False)\n",
    "# url = \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\"\n",
    "# pretrained_dict = torch.utils.model_zoo.load_url(url) \n",
    "pretrained_dict = torch.jit.load('data/RN50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeb3ef72-b72a-42e1-a103-8cd5a5f41bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r50_dict = {}\n",
    "for k, v in pretrained_dict.state_dict().items():\n",
    "    if 'visual' in k:\n",
    "        r50_dict[k.replace('visual.', '')] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0679e90a-297f-4154-9b25-5dfa92cb8b27",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv2.weight\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"conv3.weight\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\", \"attnpool.positional_embedding\", \"attnpool.k_proj.weight\", \"attnpool.k_proj.bias\", \"attnpool.q_proj.weight\", \"attnpool.q_proj.bias\", \"attnpool.v_proj.weight\", \"attnpool.v_proj.bias\", \"attnpool.c_proj.weight\", \"attnpool.c_proj.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32004/2457654333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr50_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch180/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1224\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"conv2.weight\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"conv3.weight\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\", \"attnpool.positional_embedding\", \"attnpool.k_proj.weight\", \"attnpool.k_proj.bias\", \"attnpool.q_proj.weight\", \"attnpool.q_proj.bias\", \"attnpool.v_proj.weight\", \"attnpool.v_proj.bias\", \"attnpool.c_proj.weight\", \"attnpool.c_proj.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "encoder_q.load_state_dict(r50_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938da79e-50df-4cf6-8a63-c62a5d751878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "- Duration: 30 sec\n",
      "- Image width: 224 pixels\n",
      "- Image height: 224 pixels\n",
      "- Frame rate: 30.0 fps\n",
      "- Bit rate: 129.5 Kbit/sec\n",
      "- Producer: Lavf58.29.100\n",
      "- Comment: Has audio/video index (14.1 KB)\n",
      "- MIME type: video/x-msvideo\n",
      "- Endianness: Little endian\n",
      "Video stream:\n",
      "- Duration: 30 sec\n",
      "- Image width: 224 pixels\n",
      "- Image height: 224 pixels\n",
      "- Bits/pixel: 24\n",
      "- Compression: \"FMP4\" (fourcc:\"FMP4\")\n",
      "- Frame rate: 30.0 fps\n",
      "{'Common': {'Duration': '30 sec', 'Image width': '224 pixels', 'Image height': '224 pixels', 'Frame rate': '30.0 fps', 'Bit rate': '129.5 Kbit/sec', 'Producer': 'Lavf58.29.100', 'Comment': 'Has audio/video index (14.1 KB)', 'MIME type': 'video/x-msvideo', 'Endianness': 'Little endian'}, 'Video stream': {'Duration': '30 sec', 'Image width': '224 pixels', 'Image height': '224 pixels', 'Bits/pixel': '24', 'Compression': '\"FMP4\" (fourcc:\"FMP4\")', 'Frame rate': '30.0 fps'}}\n",
      "0:00:30\n"
     ]
    }
   ],
   "source": [
    "from hachoir.metadata import extractMetadata\n",
    "from hachoir.parser import createParser\n",
    "\n",
    "def get_metadata(file_path):\n",
    "    try :\n",
    "        metadata = extractMetadata(createParser(file_path))\n",
    "        return metadata\n",
    "    except:\n",
    "        print(file_path)\n",
    "\n",
    "\n",
    "path = 'data/submission_videos_resized_224/0015d2f4c7c4f8e3b493.avi'\n",
    "metadata = get_metadata(path)\n",
    "# 输出文本格式的元数据\n",
    "print(metadata)\n",
    "# 输出字典格式的元数据\n",
    "print(metadata.exportDictionary())\n",
    "# 输出duration元数据\n",
    "print(metadata.get('duration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a65414-577a-4841-b84b-5c6b08f292ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting hachoir\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5f/7c/735cc363b85dcb96cb91de434a20f125da9cc6ea8f9fdf1d36b5f01ebafb/hachoir-3.1.3-py3-none-any.whl (647 kB)\n",
      "\u001b[K     |████████████████████████████████| 647 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: hachoir\n",
      "Successfully installed hachoir-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hachoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52d0d84-11b0-480f-bf2f-e7ae7902f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "- Duration: 30 sec\n",
      "- Image width: 224 pixels\n",
      "- Image height: 224 pixels\n",
      "- Frame rate: 30.0 fps\n",
      "- Bit rate: 139.0 Kbit/sec\n",
      "- Producer: Lavf58.29.100\n",
      "- Comment: Has audio/video index (14.1 KB)\n",
      "- MIME type: video/x-msvideo\n",
      "- Endianness: Little endian\n",
      "Video stream:\n",
      "- Duration: 30 sec\n",
      "- Image width: 224 pixels\n",
      "- Image height: 224 pixels\n",
      "- Bits/pixel: 24\n",
      "- Compression: \"FMP4\" (fourcc:\"FMP4\")\n",
      "- Frame rate: 30.0 fps\n",
      "{'Common': {'Duration': '30 sec', 'Image width': '224 pixels', 'Image height': '224 pixels', 'Frame rate': '30.0 fps', 'Bit rate': '139.0 Kbit/sec', 'Producer': 'Lavf58.29.100', 'Comment': 'Has audio/video index (14.1 KB)', 'MIME type': 'video/x-msvideo', 'Endianness': 'Little endian'}, 'Video stream': {'Duration': '30 sec', 'Image width': '224 pixels', 'Image height': '224 pixels', 'Bits/pixel': '24', 'Compression': '\"FMP4\" (fourcc:\"FMP4\")', 'Frame rate': '30.0 fps'}}\n",
      "0:00:30\n"
     ]
    }
   ],
   "source": [
    "from hachoir.metadata import extractMetadata\n",
    "from hachoir.parser import createParser\n",
    "\n",
    "def get_metadata(file_path):\n",
    "    try :\n",
    "        metadata = extractMetadata(createParser(file_path))\n",
    "        return metadata\n",
    "    except:\n",
    "        print(file_path)\n",
    "\n",
    "\n",
    "path = 'data/submission_videos_resized_224/006f0ce7d74832ae977d.avi'\n",
    "metadata = get_metadata(path)\n",
    "# 输出文本格式的元数据\n",
    "print(metadata)\n",
    "# 输出字典格式的元数据\n",
    "print(metadata.exportDictionary())\n",
    "# 输出duration元数据\n",
    "print(metadata.get('duration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0563c693-469d-4b0c-9cd4-75c1ae0e8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feat_kp_path = \"/home/sunchao/Multi_Agent_Round_2/mouse_r1_1th/results/submissions/submission.npy\"\n",
    "feat_kp = np.load(feat_kp_path, allow_pickle=True)\n",
    "np.save(feat_kp_path.replace('.npy', '_pickle_false.npy'), feat_kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa3b99c-7af5-4e5e-a05c-26ccf82bb77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22369/742312507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_kp_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_pickle_false.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch180/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 441\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch180/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    744\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "_ = np.load(feat_kp_path.replace('.npy', '_pickle_false.npy'), allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e0b20a-8091-4478-8870-402c3e0f59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feat_kp_path = \"/home/sunchao/Multi_Agent_Round_2/mouse_r1_1th/results/submissions/submission.npy\"\n",
    "feat_kp = np.load(feat_kp_path, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbfd346-3941-4a73-8530-f8cb7e745919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3294000, 59)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_kp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c1940-3f53-4841-a230-fb471ad94fe2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ce7a32-f841-472d-9560-e317ac835b5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22369/3740864121.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: f() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "def f(a, b, c):\n",
    "    return a + b + c\n",
    "\n",
    "f(1, 2, 3, 4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
